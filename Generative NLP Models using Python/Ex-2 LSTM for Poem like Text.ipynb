{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f0bfc81",
   "metadata": {},
   "source": [
    "# LSTM for Text and Sequence Generation\n",
    "This notebook explains the mathematical foundation and code implementation of LSTM (Long Short-Term Memory) models for both text generation and sequence prediction tasks using Python and TensorFlow/Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9d540c",
   "metadata": {},
   "source": [
    "## 1. What is LSTM?\n",
    "LSTM (Long Short-Term Memory) is a type of Recurrent Neural Network (RNN) that is capable of learning long-term dependencies. It solves the vanishing gradient problem in traditional RNNs using gates that control the flow of information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c8613f",
   "metadata": {},
   "source": [
    "### LSTM Cell Structure\n",
    "**Mathematics:**\n",
    "\n",
    "- Forget gate: $f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$\n",
    "- Input gate: $i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$\n",
    "- Candidate memory: $\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$\n",
    "- Output gate: $o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$\n",
    "- Final memory update: $C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t$\n",
    "- Hidden state: $h_t = o_t * \\tanh(C_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fbde5c",
   "metadata": {},
   "source": [
    "##  LSTM model to generate poem-like text. \n",
    "\n",
    "* **Temperature Sampling:** Controls randomness when generating each character. A lower temperature (e.g., 0.5) makes predictions more conservative, while a higher temperature (e.g., 1.5) introduces more randomness.\n",
    "* **Early Stopping:** A callback that stops training if the validation loss stops improving, preventing overfitting.\n",
    "* **Seed Control:** Setting random seeds for reproducibility, ensuring you get the same model initialization and training behavior each time.\n",
    "\n",
    "\n",
    "### Explanation of Key Concepts\n",
    "\n",
    "1. **Temperature Sampling:**\n",
    "\n",
    "   * **What it is:** Adjusts the probability distribution used to pick the next character by applying a \"temperature\" parameter.\n",
    "   * **Example:**\n",
    "\n",
    "     * With `temperature=0.5`, the model tends to pick high-probability characters (more deterministic).\n",
    "     * With `temperature=1.5`, choices become more random, which might lead to more creative or unexpected outputs.\n",
    "\n",
    "2. **Early Stopping:**\n",
    "\n",
    "   * **What it is:** A strategy to halt model training when further improvement is unlikely, based on monitoring a metric (e.g., loss).\n",
    "   * **Example:**\n",
    "\n",
    "     * If the training loss does not decrease for 5 consecutive epochs (`patience=5`), training stops to avoid overfitting.\n",
    "\n",
    "3. **Seed Control:**\n",
    "\n",
    "   * **What it is:** Setting fixed random seeds in Python, NumPy, and TensorFlow to ensure reproducibility.\n",
    "   * **Example:**\n",
    "\n",
    "     * By setting `seed_value=42` for all relevant libraries, you ensure that the randomness (e.g., weight initialization, training shuffles, sampling) remains the same across different runs.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "149a7397",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "np.random.seed(2)\n",
    "numbers=np.random.rand(5)\n",
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19da04ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters: ['\\n', ' ', ',', ';', 'A', 'I', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'n', 'o', 'r', 's', 't', 'u', 'v', 'w', 'y']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "seed_value = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "\n",
    "text = (\n",
    "    \"Two roads diverged in a yellow wood,\\n\"\n",
    "    \"And sorry I could not travel both\\n\"\n",
    "    \"And be one traveler, long I stood\\n\"\n",
    "    \"And looked down one as far as I could\\n\"\n",
    "    \"To where it bent in the undergrowth;\"\n",
    ")\n",
    "\n",
    "# Create a sorted list of unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "print(\"Unique characters:\", chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "895a4464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mappings from characters to indices and vice versa\n",
    "char2idx = {c: i for i, c in enumerate(chars)}\n",
    "idx2char = {i: c for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6a5a2f",
   "metadata": {},
   "source": [
    "a=[]\n",
    "a.append(\"%\")\n",
    "a.append(\"#\")\n",
    "a\n",
    "len (seq)- 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "378316ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 129\n"
     ]
    }
   ],
   "source": [
    "# Set sequence length and create input-output sequences\n",
    "seq_length = 50\n",
    "sequences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - seq_length):\n",
    "    sequences.append(text[i: i + seq_length])\n",
    "    next_chars.append(text[i + seq_length])\n",
    "\n",
    "print(\"Number of sequences:\", len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6fd8ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the sequences (one-hot encoding)\n",
    "X = np.zeros((len(sequences), seq_length, len(chars)), dtype=np.bool_)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=np.bool_)\n",
    "\n",
    "for i, seq in enumerate(sequences):\n",
    "    for t, char in enumerate(seq):\n",
    "        X[i, t, char2idx[char]] = 1\n",
    "    y[i, char2idx[next_chars[i]]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f9fc25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Suyashi144893\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">79,872</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,483</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m79,872\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m)                  │           \u001b[38;5;34m3,483\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">83,355</span> (325.61 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m83,355\u001b[0m (325.61 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">83,355</span> (325.61 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m83,355\u001b[0m (325.61 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We define a simple LSTM model for character-level text prediction.\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(128, input_shape=(seq_length, len(chars))),\n",
    "    Dense(len(chars), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd78ee3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Early Stopping** is a strategy that monitors a metric (e.g., validation loss) during training,\n",
    "# and stops training if there is no further improvement. This can save time and prevent overfitting.\n",
    "# In our case, we monitor the training loss (or you could split some data as a validation set)\n",
    "# and stop if it does not improve for a few epochs.\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03d25c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 3.2847\n",
      "Epoch 2/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.1725\n",
      "Epoch 3/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.8910\n",
      "Epoch 4/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2.8720\n",
      "Epoch 5/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2.8336\n",
      "Epoch 6/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2.8184\n",
      "Epoch 7/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2.8081\n",
      "Epoch 8/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2.7935\n",
      "Epoch 9/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.7778\n",
      "Epoch 10/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2.7612\n",
      "Epoch 11/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.7391\n",
      "Epoch 12/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2.7089\n",
      "Epoch 13/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2.6813\n",
      "Epoch 14/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.6633\n",
      "Epoch 15/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.6901\n",
      "Epoch 16/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.6465\n",
      "Epoch 17/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.6687\n",
      "Epoch 18/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2.6057\n",
      "Epoch 19/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.5719\n",
      "Epoch 20/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.5109\n",
      "Epoch 21/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2.4575\n",
      "Epoch 22/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.3987\n",
      "Epoch 23/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2.4858\n",
      "Epoch 24/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2.4149\n",
      "Epoch 25/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2.3499\n",
      "Epoch 26/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2.2135\n",
      "Epoch 27/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.1242\n",
      "Epoch 28/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.0749\n",
      "Epoch 29/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.9952\n",
      "Epoch 30/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.9352\n",
      "Epoch 31/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2.0289\n",
      "Epoch 32/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.9452\n",
      "Epoch 33/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.0856\n",
      "Epoch 34/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.1436\n",
      "Epoch 35/50\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2.0565\n",
      "Epoch 35: early stopping\n",
      "Generated poem with temperature=0.5:\n",
      "\n",
      "Seed: Two roads diverged in a yellow wood,\n",
      "And sorry I c\n",
      "Two roads diverged in a yellow wood,\n",
      "And sorry I c nn d   asasaaI s\n",
      "Ao\n",
      "en ie e   avvvevel        tgr o;b\n",
      "noondno o oen avvevrl          od\n",
      "wo ne  en h n  vvea r I  s \n",
      "doodowkoooelw een n  vravaaav    Icote\n",
      "Tn oetiehee bnen nnvtvvrrvvrggg;;,odowAnoood\n",
      "\n",
      "Generated poem with temperature=1.0:\n",
      "\n",
      "Seed: Two roads diverged in a yellow wood,\n",
      "And sorry I c\n",
      "Two roads diverged in a yellow wood,\n",
      "And sorry I ce  obd     salu rtu d nnt ii nte evavurn dd e orrggeravduwhooddteToneeen a vevv Ie  a tr\n",
      "sgowdtT dAdoodoben awe v dv nnsIl rIs l nodkw ndedereee  aivaaev esrrtrIIhAwodddd enb ose eo  no v oref  no tdo\n",
      "\n",
      "Generated poem with temperature=1.5:\n",
      "\n",
      "Seed: Two roads diverged in a yellow wood,\n",
      "And sorry I c\n",
      "Two roads diverged in a yellow wood,\n",
      "And sorry I coluln  loaavgrnIsAcd\n",
      "bTgnkrrhtvvelaavnoreo;I uf;\n",
      "gt,;A\n",
      "b;nrvThlkdke e r wfassgIoIlv\n",
      "sed\n",
      "\n",
      "toAwerdn ht,end b dataerlsrrIldodsdI,colrAddkndotoaoaen raavteo  ausIg AsuudbkkTyAow n\n",
      "tb iIt tnoo awslIn toolh\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## 5. Training the Model\n",
    "# We train the model on our prepared dataset. For real applications, use more epochs and a larger corpus.\n",
    "\n",
    "\n",
    "history = model.fit(X, y, epochs=50, batch_size=16, callbacks=[early_stopping])\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Sample an index from a probability array reweighted by temperature.\n",
    "    \"\"\"\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds + 1e-8) / temperature  # add epsilon to avoid log(0)\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def generate_text(seed, length=200, temperature=1.0):\n",
    "    generated = seed\n",
    "    print(\"Seed:\", seed)\n",
    "    for i in range(length):\n",
    "        # Prepare the input sequence (one-hot encoding)\n",
    "        x_pred = np.zeros((1, seq_length, len(chars)))\n",
    "        for t, char in enumerate(seed):\n",
    "            x_pred[0, t, char2idx[char]] = 1.\n",
    "        \n",
    "        # Predict the next character probabilities\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature)\n",
    "        next_char = idx2char[next_index]\n",
    "        \n",
    "        # Append the next character\n",
    "        generated += next_char\n",
    "        seed = seed[1:] + next_char\n",
    "    return generated\n",
    "\n",
    "\n",
    "seed_text = text[:seq_length]\n",
    "print(\"Generated poem with temperature=0.5:\\n\")\n",
    "print(generate_text(seed_text, length=200, temperature=0.5))\n",
    "\n",
    "print(\"\\nGenerated poem with temperature=1.0:\\n\")\n",
    "print(generate_text(seed_text, length=200, temperature=1.0))\n",
    "\n",
    "print(\"\\nGenerated poem with temperature=1.5:\\n\")\n",
    "print(generate_text(seed_text, length=200, temperature=1.5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12dec84",
   "metadata": {},
   "source": [
    "### Why not meaningful Output: \n",
    "Use a large corpus of poems (e.g., thousands of Shakespearean sonnets, modern poems, etc.) for better learning.\n",
    "### Lack of Meaning Understanding in LSTM\n",
    "- LSTMs don't “understand” meaning—they only learn statistical patterns of sequences.\n",
    "\n",
    "- For actual semantic understanding or theme, you'd need Transformers (GPT, BERT) trained on large corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538bfc97",
   "metadata": {},
   "source": [
    "| Improvement               | What to Do                                                          |\n",
    "| ------------------------- | ------------------------------------------------------------------- |\n",
    "| **More Data**             | Use a large text corpus (\\~1MB or more) of poems.                   |\n",
    "| **Word-Level Modeling**   | Use `Tokenizer` + `Embedding` layer for word-level LSTM generation. |\n",
    "| **Train Longer**          | Use at least 100–200 epochs with good hardware.                     |\n",
    "| **Use GRU/BiLSTM**        | Try stacking layers or using bidirectional LSTMs.                   |\n",
    "| **Use Pretrained Models** | Fine-tune GPT-2 or LLaMA models on your poem dataset.               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57873df1",
   "metadata": {},
   "source": [
    "## Fine-tuning a small GPT model on poems will result in meaningful poetic generation much faster than an LSTM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
