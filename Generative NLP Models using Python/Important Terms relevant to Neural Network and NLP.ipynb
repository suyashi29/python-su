{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a402ea3",
   "metadata": {},
   "source": [
    "Data Science is interesting Topic.Data Analytics , Data Lover and Data Science. \n",
    "\n",
    "Science - \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814e7d68",
   "metadata": {},
   "source": [
    "\n",
    "## 1. N-grams\n",
    "\n",
    "**Definition:**\n",
    "An *n-gram* is a contiguous sequence of *n* tokens (characters or words) extracted from text.\n",
    "\n",
    "**Why it matters:**\n",
    "\n",
    "* Historically used in statistical language models.\n",
    "* Conceptually helps understand how models learn local context.\n",
    "* Still useful in preprocessing, evaluation, and feature engineering.\n",
    "\n",
    "**Examples (word-level):**\n",
    "\n",
    "* Unigram (n=1): `\"I\"`, `\"love\"`, `\"AI\"`\n",
    "* Bigram (n=2): `\"I love\"`, `\"love AI\"`\n",
    "* Trigram (n=3): `\"I love AI\"`\n",
    "\n",
    "**In neural models:**\n",
    "\n",
    "* **LSTM/RNN:** Implicitly learns variable-length n-gram–like dependencies over time.\n",
    "* **Transformers:** Attention mechanisms learn relationships beyond fixed n-grams, including long-range dependencies.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Temperature (in Text Generation)\n",
    "\n",
    "**Definition:**\n",
    "*Temperature* controls the randomness of token selection during text generation by scaling the logits before applying softmax.\n",
    "\n",
    "**Formula intuition:**\n",
    "\n",
    "```\n",
    "softmax(logits / temperature)\n",
    "```\n",
    "\n",
    "**Effect of temperature:**\n",
    "\n",
    "* **Low temperature (< 1.0):** More deterministic, safer outputs\n",
    "* **High temperature (> 1.0):** More diverse, creative, riskier outputs\n",
    "\n",
    "**Example:**\n",
    "If predicted probabilities for next word are:\n",
    "\n",
    "```\n",
    "{\"AI\": 0.6, \"ML\": 0.25, \"Data\": 0.15}\n",
    "```\n",
    "\n",
    "* Temperature = **0.5** → likely always selects `\"AI\"`\n",
    "* Temperature = **1.5** → `\"ML\"` or `\"Data\"` may appear more often\n",
    "\n",
    "**Usage:**\n",
    "\n",
    "* Common in **LSTM text generators** and **Transformer decoders**\n",
    "* Often combined with **top-k** or **top-p (nucleus) sampling**\n",
    "\n",
    "---\n",
    "\n",
    "## 3. BATCH_SIZE\n",
    "\n",
    "**Definition:**\n",
    "The number of training samples processed together in one forward/backward pass.\n",
    "\n",
    "**Why it matters:**\n",
    "\n",
    "* Affects training speed, memory usage, and gradient stability.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* Dataset size: 10,000 sentences\n",
    "* Batch size = 32 → ~313 batches per epoch\n",
    "\n",
    "**Model perspective:**\n",
    "\n",
    "* **LSTM:** Smaller batches often preferred for stability.\n",
    "* **Transformers:** Larger batch sizes are common (with GPUs/TPUs).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. BUFFER_SIZE\n",
    "\n",
    "**Definition:**\n",
    "The number of samples held in memory while shuffling the dataset (common in TensorFlow pipelines).\n",
    "\n",
    "**Why it matters:**\n",
    "\n",
    "* Larger buffer → better randomization\n",
    "* Smaller buffer → faster, but less shuffled data\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "dataset.shuffle(BUFFER_SIZE=10000)\n",
    "```\n",
    "\n",
    "**Impact:**\n",
    "\n",
    "* Improves generalization by preventing sequence bias.\n",
    "* Important when training language models on ordered text.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Shuffle\n",
    "\n",
    "**Definition:**\n",
    "Randomly rearranging training samples before batching.\n",
    "\n",
    "**Why it matters in NLP:**\n",
    "\n",
    "* Prevents the model from learning artificial ordering.\n",
    "* Reduces overfitting to sequential patterns in the dataset.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "* Shuffling happens at the **sequence level**, not within a sentence.\n",
    "* In Transformers, shuffling does *not* affect positional encodings inside sequences.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Embedding (Embedding Layer)\n",
    "\n",
    "**Definition:**\n",
    "A trainable mapping from discrete tokens (words/subwords) to dense numerical vectors.\n",
    "\n",
    "**Why embeddings are critical:**\n",
    "\n",
    "* Neural networks cannot operate on raw text.\n",
    "* Embeddings capture semantic and syntactic relationships.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```\n",
    "\"king\" → [0.21, -0.45, 0.88, ...]\n",
    "\"queen\" → [0.19, -0.42, 0.91, ...]\n",
    "```\n",
    "\n",
    "**Model usage:**\n",
    "\n",
    "* **LSTM:** Embedding → LSTM → Dense\n",
    "* **Transformers:** Token embedding + Positional embedding → Attention blocks\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Parameters\n",
    "\n",
    "**Definition:**\n",
    "Trainable values learned by the model during training.\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "* Embedding matrices\n",
    "* LSTM weights and biases\n",
    "* Attention projection matrices in Transformers\n",
    "\n",
    "**Scale comparison:**\n",
    "\n",
    "* LSTM language model: Thousands to millions of parameters\n",
    "* Transformer models: Millions to billions of parameters\n",
    "\n",
    "**Why it matters:**\n",
    "\n",
    "* More parameters → higher capacity\n",
    "* Also higher risk of overfitting and higher compute cost\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Epoch\n",
    "\n",
    "**Definition:**\n",
    "One complete pass through the entire training dataset.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* Dataset: 10,000 samples\n",
    "* Batch size: 100\n",
    "* 1 epoch = 100 gradient updates\n",
    "\n",
    "**In practice:**\n",
    "\n",
    "* Text models typically train for multiple epochs.\n",
    "* Transformers may converge in fewer epochs due to scale.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Vocabulary (Vocab)\n",
    "\n",
    "**Definition:**\n",
    "The set of unique tokens the model can understand and generate.\n",
    "\n",
    "**Types of tokens:**\n",
    "\n",
    "* Character-level\n",
    "* Word-level\n",
    "* Subword-level (BPE, WordPiece, SentencePiece)\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```\n",
    "Vocab size = 30,000 tokens\n",
    "```\n",
    "\n",
    "**Impact:**\n",
    "\n",
    "* Larger vocab → richer language, more parameters\n",
    "* Smaller vocab → more token splitting, longer sequences\n",
    "\n",
    "**Transformers:**\n",
    "Almost always use **subword vocabularies**.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Early Stopping\n",
    "\n",
    "**Definition:**\n",
    "A training strategy that stops training when validation performance stops improving.\n",
    "\n",
    "**Why it matters:**\n",
    "\n",
    "* Prevents overfitting\n",
    "* Saves compute time\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* Monitor validation loss\n",
    "* Stop training if no improvement for 3 consecutive epochs\n",
    "\n",
    "**Typical usage:**\n",
    "\n",
    "* Common in **LSTM-based models**\n",
    "* Less common in very large Transformers, but still used in fine-tuning\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Mapping (Quick Reference)\n",
    "\n",
    "| Term           | Primary Purpose                   |\n",
    "| -------------- | --------------------------------- |\n",
    "| N-grams        | Local context modeling intuition  |\n",
    "| Temperature    | Controls randomness in generation |\n",
    "| Batch Size     | Training efficiency and stability |\n",
    "| Buffer Size    | Quality of data shuffling         |\n",
    "| Shuffle        | Prevents order bias               |\n",
    "| Embedding      | Converts tokens to vectors        |\n",
    "| Parameters     | Model capacity                    |\n",
    "| Epoch          | Training progress unit            |\n",
    "| Vocabulary     | Language coverage                 |\n",
    "| Early Stopping | Prevents overfitting              |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06115d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
