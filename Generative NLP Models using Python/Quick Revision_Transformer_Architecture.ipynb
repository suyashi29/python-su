{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f43ecaf",
   "metadata": {},
   "source": [
    "\n",
    "# Understanding Transformer Architecture – End-to-End\n",
    "### Trainer & Learner Friendly Jupyter Notebook\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a3c7a3",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Why Transformers?\n",
    "### Limitations of RNNs / LSTMs\n",
    "\n",
    "• Sequential computation → slow training  \n",
    "• Long-term dependencies are hard to learn  \n",
    "• Vanishing / exploding gradients  \n",
    "• Limited parallelism  \n",
    "\n",
    "**Transformers solve this using Attention.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b831c4",
   "metadata": {},
   "source": [
    "\n",
    "## 2. High-Level Transformer Architecture\n",
    "\n",
    "A Transformer consists of:\n",
    "- Encoder Stack\n",
    "- Decoder Stack\n",
    "- Attention Mechanism\n",
    "- Positional Encoding\n",
    "\n",
    "Each block is built using **Attention + Feed Forward Networks**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64796407",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Tokenization & Embeddings\n",
    "\n",
    "Before entering the Transformer:\n",
    "1. Text → Tokens (word/subword)\n",
    "2. Tokens → Token IDs\n",
    "3. Token IDs → Embedding vectors\n",
    "\n",
    "Embedding captures semantic meaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c737232",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "vocab_size = 10000\n",
    "embedding_dim = 512\n",
    "\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "sample_tokens = torch.tensor([10, 25, 300])\n",
    "embedding(sample_tokens).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc80ca6f",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Positional Encoding\n",
    "Transformers do NOT have sequence awareness.\n",
    "\n",
    "Positional Encoding adds order information.\n",
    "\n",
    "### Formula:\n",
    "PE(pos, 2i)   = sin(pos / 10000^(2i/d))\n",
    "PE(pos, 2i+1) = cos(pos / 10000^(2i/d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0687ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "          1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "        [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
       "          9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
       "        [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
       "          9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
       "        [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9996e-02,\n",
       "          9.9955e-01,  3.0000e-03,  1.0000e+00],\n",
       "        [-7.5680e-01, -6.5364e-01,  3.8942e-01,  9.2106e-01,  3.9989e-02,\n",
       "          9.9920e-01,  4.0000e-03,  9.9999e-01]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import math\n",
    "import torch\n",
    "\n",
    "def positional_encoding(seq_len, d_model):\n",
    "    pe = torch.zeros(seq_len, d_model)\n",
    "    for pos in range(seq_len):\n",
    "        for i in range(0, d_model, 2):\n",
    "            pe[pos, i] = math.sin(pos / (10000 ** (i / d_model)))\n",
    "            pe[pos, i+1] = math.cos(pos / (10000 ** (i / d_model)))\n",
    "    return pe\n",
    "\n",
    "positional_encoding(5, 8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca8112e",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Self-Attention – Core Idea\n",
    "\n",
    "Each word attends to every other word.\n",
    "\n",
    "We compute:\n",
    "- Query (Q)\n",
    "- Key (K)\n",
    "- Value (V)\n",
    "\n",
    "Attention(Q,K,V) = softmax(QKᵀ / √d) V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a471129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 8]), torch.Size([1, 4, 4]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    d_k = Q.size(-1)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    weights = torch.softmax(scores, dim=-1)\n",
    "    return torch.matmul(weights, V), weights\n",
    "\n",
    "Q = torch.rand(1, 4, 8)\n",
    "K = torch.rand(1, 4, 8)\n",
    "V = torch.rand(1, 4, 8)\n",
    "\n",
    "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "output.shape, attn_weights.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336bea2f",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Multi-Head Attention\n",
    "\n",
    "Instead of one attention:\n",
    "- Split embeddings into multiple heads\n",
    "- Each head learns different relationships\n",
    "- Concatenate results\n",
    "\n",
    "This improves representational power.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a747156",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Feed Forward Network (FFN)\n",
    "\n",
    "Applied independently to each token.\n",
    "\n",
    "Structure:\n",
    "Linear → ReLU → Linear\n",
    "\n",
    "Adds non-linearity and depth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f7eadc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 512])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "ffn = nn.Sequential(\n",
    "    nn.Linear(512, 2048),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(2048, 512)\n",
    ")\n",
    "\n",
    "ffn(torch.rand(2, 5, 512)).shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30a7b48",
   "metadata": {},
   "source": [
    "**nn.Linear operates on the last dimension of the input tensor. Here, each of the 512-dimensional vectors in the (2, 5) sequence is independently passed through the feed-forward network, preserving the batch and sequence dimensions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b0e515",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Residual Connections & Layer Normalization\n",
    "\n",
    "Why?\n",
    "• Stabilize training  \n",
    "• Faster convergence  \n",
    "• Better gradient flow  \n",
    "\n",
    "Each sub-layer:\n",
    "Output = LayerNorm(x + Sublayer(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a415d8",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Encoder Block Summary\n",
    "\n",
    "Each Encoder layer contains:\n",
    "1. Multi-Head Self-Attention\n",
    "2. Add & Norm\n",
    "3. Feed Forward Network\n",
    "4. Add & Norm\n",
    "\n",
    "Repeated N times.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da292044",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Decoder Block Summary\n",
    "\n",
    "Decoder adds:\n",
    "- Masked Self-Attention\n",
    "- Encoder–Decoder Attention\n",
    "\n",
    "Used for text generation tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedcf3ab",
   "metadata": {},
   "source": [
    "\n",
    "## 11. Masked Attention (Why?)\n",
    "\n",
    "Prevents model from seeing future tokens during training.\n",
    "\n",
    "Essential for autoregressive generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0910e6",
   "metadata": {},
   "source": [
    "\n",
    "## 12. Transformer for Text Generation\n",
    "\n",
    "Workflow:\n",
    "1. Input tokens\n",
    "2. Encoder builds contextual representations\n",
    "3. Decoder predicts next token\n",
    "4. Sampling (temperature, top-k, top-p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b23561",
   "metadata": {},
   "source": [
    "\n",
    "## 13. Transformer vs LSTM (Intuition)\n",
    "\n",
    "| Aspect | LSTM | Transformer |\n",
    "|------|------|-------------|\n",
    "| Parallelism | ❌ | ✅ |\n",
    "| Long Context | Limited | Strong |\n",
    "| Training Speed | Slow | Fast |\n",
    "| Attention | Optional | Core |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac6e062",
   "metadata": {},
   "source": [
    "\n",
    "## 14. Connection to Large Language Models (LLMs)\n",
    "\n",
    "LLMs are:\n",
    "• Decoder-only Transformers  \n",
    "• Trained on massive corpora  \n",
    "• Predict next token  \n",
    "\n",
    "Examples:\n",
    "GPT, LLaMA, PaLM, Claude\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f532b62",
   "metadata": {},
   "source": [
    "\n",
    "## 15. Key Takeaways\n",
    "\n",
    "• Attention replaces recurrence  \n",
    "• Positional encoding adds order  \n",
    "• Multi-head attention captures rich context  \n",
    "• Transformers scale efficiently  \n",
    "• Foundation of modern GenAI\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
