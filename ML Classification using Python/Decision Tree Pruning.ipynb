{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfb52cee",
   "metadata": {},
   "source": [
    "# Decision Trees and Pruning\n",
    "\n",
    "Decision Trees are powerful models used for classification and regression. They split data into subsets based on feature values, creating a tree-like structure. However, fully grown trees can overfit the training data. **Pruning** helps reduce complexity and improve generalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9913df",
   "metadata": {},
   "source": [
    "## How Decision Trees Work\n",
    "\n",
    "Decision Trees split nodes based on impurity measures such as **Gini Index** or **Entropy**.\n",
    "\n",
    "### Formulas:\n",
    "- **Gini Index**:\n",
    "\\[\n",
    "Gini = 1 - \\sum_{i=1}^{C} p_i^2\n",
    "\\]\n",
    "\n",
    "- **Entropy**:\n",
    "\\[\n",
    "Entropy = - \\sum_{i=1}^{C} p_i \\log_2(p_i)\n",
    "\\]\n",
    "\n",
    "Where \\(p_i\\) is the proportion of class \\(i\\) in the node.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe6997d",
   "metadata": {},
   "source": [
    "## Pruning in Decision Trees\n",
    "\n",
    "Pruning reduces the size of a decision tree by removing branches that have little predictive power.\n",
    "\n",
    "### Types of Pruning:\n",
    "1. **Pre-Pruning (Early Stopping)**: Stop tree growth early using constraints like `max_depth`, `min_samples_split`.\n",
    "2. **Post-Pruning (Cost Complexity Pruning)**: Grow the full tree, then prune back.\n",
    "\n",
    "### Cost Complexity Formula:\n",
    "\\[\n",
    "R_\\alpha(T) = R(T) + \\alpha |T|\n",
    "\\]\n",
    "Where:\n",
    "- \\(R(T)\\): Misclassification error of tree \\(T\\)\n",
    "- \\(|T|\\): Number of leaves\n",
    "- \\(\\alpha\\): Complexity parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42390004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Simple tree before pruning\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from([(\"Root\", \"Node A\"), (\"Root\", \"Node B\"), (\"Node A\", \"Leaf 1\"), (\"Node A\", \"Leaf 2\"), (\"Node B\", \"Leaf 3\"), (\"Node B\", \"Leaf 4\")])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=2000, font_size=12)\n",
    "plt.title('Decision Tree Before Pruning')\n",
    "plt.show()\n",
    "\n",
    "# After pruning\n",
    "G_pruned = nx.DiGraph()\n",
    "G_pruned.add_edges_from([(\"Root\", \"Node A\"), (\"Root\", \"Node B\"), (\"Node A\", \"Leaf 1\"), (\"Node B\", \"Leaf 3\")])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "pos = nx.spring_layout(G_pruned)\n",
    "nx.draw(G_pruned, pos, with_labels=True, node_color='lightgreen', node_size=2000, font_size=12)\n",
    "plt.title('Decision Tree After Pruning')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697211e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train full tree\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print('Accuracy (Full Tree):', clf.score(X_test, y_test))\n",
    "\n",
    "# Pruned tree using cost complexity pruning\n",
    "clf_pruned = DecisionTreeClassifier(random_state=42, ccp_alpha=0.02)\n",
    "clf_pruned.fit(X_train, y_train)\n",
    "print('Accuracy (Pruned Tree):', clf_pruned.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4befa251",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Pruning helps prevent overfitting by simplifying the tree structure. It improves generalization and makes the model more interpretable.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
